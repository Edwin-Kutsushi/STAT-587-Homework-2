{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c933955a-181d-4f1e-898f-73d68c8c14cf",
   "metadata": {},
   "source": [
    "## Consider the College data from the ISLP package. Details about the data is described on page 65 of the ISLP textbook for this class (https://islp.readthedocs.io/en/latest/datasets/College.html). We would like to predict the number of applications received using the other variables. 80% of the data (randomly generated) will be treated as training data. The rest will be the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1a2690-42e0-4838-9b57-6e24d7cafaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# importlib.util allows us to check whether a package is already installed\n",
    "import importlib.util\n",
    "\n",
    "# Check if the ISLP package is available in the current Jupyter kernel\n",
    "if importlib.util.find_spec(\"ISLP\") is None:\n",
    "    # Install ISLP if it is not already installed\n",
    "    !pip install ISLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b41e5-0016-4876-bd56-f16e85e1c2ca",
   "metadata": {},
   "source": [
    "### a) Fit a linear model using least squares and report the estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce2210c-8e00-461f-83a7-afc836f995ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load college dataset\n",
    "college = load_data(\"College\")\n",
    "\n",
    "# Split the data into 80% training and 20% test sets\n",
    "train_df, test_df = train_test_split(college, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear model using least squares (OLS)\n",
    "# Remove response from predictors\n",
    "predictors = train_df.columns.drop(\"Apps\")\n",
    "\n",
    "design = MS(predictors)\n",
    "design = design.fit(train_df)\n",
    "\n",
    "# Create the design matrices for training and testing data\n",
    "X_train = design.transform(train_df)\n",
    "y_train = train_df[\"Apps\"]\n",
    "X_test = design.transform(test_df)\n",
    "y_test = test_df[\"Apps\"]\n",
    "\n",
    "# Fit the OLS model using statsmodels\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "\n",
    "# Report the estimate of the test error (Mean Squared Error)\n",
    "# Make predictions on the test set\n",
    "predictions = results.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) on the test set\n",
    "test_error = mean_squared_error(y_test, predictions)\n",
    "\n",
    "#print(f\"Estimated test MSE: {test_error:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0c45e-bce6-4964-b0eb-e8d78454deef",
   "metadata": {},
   "source": [
    "### b) Fit a tree to the data. Summarize the results. Unless the number of terminal nodes is large, display the tree graphically. Report its MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1001f9f-3f7d-4bb0-adac-3f070c8b69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "# Encode categorical variable\n",
    "college = pd.get_dummies(college, columns=[\"Private\"], drop_first=True)\n",
    "college[\"Private_Yes\"] = college[\"Private_Yes\"].astype(int)\n",
    "\n",
    "# Split predictors and response\n",
    "X = college.drop(\"Apps\", axis=1)\n",
    "y = college[\"Apps\"]\n",
    "\n",
    "# Train‚Äìtest split (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit a regression tree\n",
    "#    max_depth=3 keeps the tree interpretable\n",
    "# tree_model = DecisionTreeRegressor(\n",
    "#     max_depth=3,\n",
    "#     random_state=42\n",
    "# )\n",
    "# tree_model.fit(X_train, y_train)\n",
    "\n",
    "tree_model = DecisionTreeRegressor(\n",
    "    ccp_alpha=0.0,\n",
    "    random_state=42\n",
    ")\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict and compute test MSE\n",
    "y_pred = tree_model.predict(X_test)\n",
    "tree_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(f\"Regression Tree Test MSE: {tree_mse:,.2f}\")\n",
    "\n",
    "# Display the tree graphically\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plot_tree(\n",
    "#     tree_model,\n",
    "#     feature_names=X.columns.tolist(),\n",
    "#     filled=True,\n",
    "#     rounded=True,\n",
    "#     impurity=False,\n",
    "#     label=\"all\",\n",
    "#     node_ids=True,\n",
    "#     fontsize=10\n",
    "# )\n",
    "\n",
    "#plt.title(\"Regression Tree (Max Depth = 3)\")\n",
    "#plt.savefig(\"Regression Tree.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5b028-46c2-436d-a63e-37029a94d3fa",
   "metadata": {},
   "source": [
    "### c) Use Cross validation to determine whether pruning is helpful and determine the optimal size for the pruned tree. Compare the pruned and un-pruned trees. Report MSE for the pruned tree. Which predictors seem to be the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead4eeb8-c3ad-49ae-91b8-47ae364127cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Encode categorical variable\n",
    "# college = pd.get_dummies(college, columns=[\"Private\"], drop_first=True)\n",
    "# college[\"Private_Yes\"] = college[\"Private_Yes\"].astype(int)\n",
    "\n",
    "# X = college.drop(\"Apps\", axis=1)\n",
    "# y = college[\"Apps\"]\n",
    "\n",
    "# # Train‚Äìtest split (80% / 20%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# Cost-complexity pruning path\n",
    "base_tree = DecisionTreeRegressor(random_state=42)\n",
    "path = base_tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Cross-validation to select optimal ccp_alpha\n",
    "param_grid = {\"ccp_alpha\": ccp_alphas}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "optimal_ccp_alpha = grid.best_params_[\"ccp_alpha\"]\n",
    "#print(f\"Optimal ccp_alpha: {optimal_ccp_alpha}\")\n",
    "\n",
    "# Fit un-pruned tree (baseline)\n",
    "unpruned_tree = DecisionTreeRegressor(ccp_alpha=0.0, random_state=42)\n",
    "unpruned_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_unpruned = unpruned_tree.predict(X_test)\n",
    "mse_unpruned = mean_squared_error(y_test, y_pred_unpruned)\n",
    "\n",
    "# Fit pruned tree (CV-selected)\n",
    "pruned_tree = DecisionTreeRegressor(\n",
    "    ccp_alpha=optimal_ccp_alpha,\n",
    "    random_state=42\n",
    ")\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pruned = pruned_tree.predict(X_test)\n",
    "mse_pruned = mean_squared_error(y_test, y_pred_pruned)\n",
    "\n",
    "# Report MSEs\n",
    "# print(f\"\\nUn-pruned Tree Test MSE: {mse_unpruned:,.2f}\")\n",
    "# print(f\"Pruned Tree Test MSE:   {mse_pruned:,.2f}\")\n",
    "\n",
    "# Important predictors (from pruned tree)\n",
    "feature_importance = (\n",
    "    pd.Series(pruned_tree.feature_importances_, index=X.columns)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# print(\"\\nMost Important Predictors (Pruned Tree):\")\n",
    "# print(feature_importance.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c0381-69fd-4535-b18e-69cbf10ab10a",
   "metadata": {},
   "source": [
    "### d) Use a bagging approach to analyze the data with B = 500 and B = 1000. Compute the MSE. Which predictors seem to be the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3622b8-b175-48d7-bca4-9c1e5fc6038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# The variable 'Private' is categorical (Yes/No).\n",
    "# For tree-based methods, it is sufficient to encode it as a binary variable (0/1).\n",
    "# college[\"Private\"] = college[\"Private\"].astype(\"category\").cat.codes\n",
    "\n",
    "\n",
    "# Define predictors and response\n",
    "X = college.drop(\"Apps\", axis=1)   # Predictor variables\n",
    "y = college[\"Apps\"]                # Response variable\n",
    "feature_names = X.columns          # Save feature names for interpretation\n",
    "\n",
    "\n",
    "# Train‚Äìtest split (80% training, 20% test)\n",
    "# A fixed random_state ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Bagging function\n",
    "# This function fits a BaggingRegressor with a specified\n",
    "# number of trees (n_estimators) and returns the test MSE.\n",
    "def evaluate_bagging(n_estimators, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Bagging uses bootstrap samples and averages predictions\n",
    "    # max_samples=1.0 means each bootstrap sample has the same size as the training set\n",
    "    bag_model = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),  # base learner\n",
    "        n_estimators=n_estimators,           # number of trees\n",
    "        max_samples=1.0,                     # bootstrap sample size\n",
    "        bootstrap=True,                      # sample with replacement\n",
    "        random_state=42,\n",
    "        n_jobs=-1                            # use all available CPU cores\n",
    "    )\n",
    "    \n",
    "    # Fit the bagging model\n",
    "    bag_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = bag_model.predict(X_test)\n",
    "    \n",
    "    # Compute test Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return mse, bag_model\n",
    "\n",
    "\n",
    "# Evaluate bagging with different numbers of trees\n",
    "# Compare performance as the number of trees increases\n",
    "mse_500, model_500 = evaluate_bagging(\n",
    "    500, X_train, y_train, X_test, y_test\n",
    ")\n",
    "# print(f\"Test MSE with B = 500 trees: {mse_500:.2f}\")\n",
    "\n",
    "mse_1000, model_1000 = evaluate_bagging(\n",
    "    1000, X_train, y_train, X_test, y_test\n",
    ")\n",
    "# print(f\"Test MSE with B = 1000 trees: {mse_1000:.2f}\")\n",
    "\n",
    "\n",
    "# Identify important predictors\n",
    "rf_bagging = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_features=None,   # use all predictors at each split (bagging)\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_bagging.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"importance\": rf_bagging.feature_importances_},\n",
    "    index=feature_names\n",
    ").sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# print(\"\\nMost Important Predictors:\")\n",
    "# print(feature_importance)\n",
    "\n",
    "# # Plot feature importance\n",
    "# feature_importance.plot(kind=\"bar\", legend=False)\n",
    "# plt.title(\"Feature Importance for Predicting College Applications\")\n",
    "# plt.ylabel(\"Importance (Mean Decrease in Impurity)\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43400e43-b7af-4daf-bb08-0e8aa4b999f8",
   "metadata": {},
   "source": [
    "### e) Repeat (d) with a random forest approach with B = 500 and B = 1000, and m ‚âà p = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8760e6f-23d5-42dc-88ed-db2e2dd7c547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "\n",
    "# # Load the College data\n",
    "# college = load_data(\"College\")\n",
    "\n",
    "# # Convert 'Private' (Yes/No) to numeric (0/1)\n",
    "# college[\"Private\"] = college[\"Private\"].factorize()[0]\n",
    "\n",
    "# # Define predictors and response\n",
    "# X = college.drop(\"Apps\", axis=1)\n",
    "# y = college[\"Apps\"]\n",
    "\n",
    "# # Train‚Äìtest split (80% / 20%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# Set Random Forest parameters\n",
    "p = X_train.shape[1]     # number of predictors\n",
    "m = 3                   # user-specified m ‚âà p/3\n",
    "\n",
    "# print(f\"Number of predictors (p): {p}\")\n",
    "# print(f\"Using m = {m} predictors at each split.\\n\")\n",
    "\n",
    "# Function to train and evaluate Random Forest\n",
    "def train_and_evaluate_rf(n_estimators, max_features):\n",
    "    \n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,   # B\n",
    "        max_features=max_features,   # m\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # print(f\"Random Forest (B = {n_estimators}, m = {max_features})\")\n",
    "    # print(f\"  Test MSE : {mse:,.2f}\")\n",
    "    # print(f\"  Test RMSE: {rmse:,.2f}\\n\")\n",
    "    \n",
    "    return rf_model, mse\n",
    "\n",
    "\n",
    "# Fit models for B = 500 and B = 1000\n",
    "# rf_500, mse_500 = train_and_evaluate_rf(500, m)\n",
    "# rf_1000, mse_1000 = train_and_evaluate_rf(1000, m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42ae53-0859-4b0c-aa96-6b7ad1d0586f",
   "metadata": {},
   "source": [
    "## 2. Consider the business school admission data available in the admission.csv. The admission officer of a business school has used an ‚Äúindex‚Äù of undergraduate grade point average (GPA,ùëã1) and graduate management aptitude test (GMAT,ùëã2) scores to help decide which applicants should be admitted to the school‚Äôs graduate programs. This index is used to categorize each applicant into one of three groups ‚Äì admit (group 1), do not admit (group 2), and borderline (group 3). We will take the last four observations in each category as test data and the remaining observations as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9a1d6-c644-4ab1-9a99-f7eefbdd2f0c",
   "metadata": {},
   "source": [
    "### a) Perform an exploratory analysis of the training data by examining appropriate plots and comment on how helpful these predictors may be in predicting response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73283337-1618-430f-a8d5-03ec4b5e9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "admission = pd.read_csv(\"admission.csv\")\n",
    "\n",
    "# Split into training and test data\n",
    "# last 4 observations in each group are test data\n",
    "\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for _, group_df in admission.groupby(\"Group\"):\n",
    "    train_parts.append(group_df.iloc[:-4])\n",
    "    test_parts.append(group_df.iloc[-4:])\n",
    "\n",
    "train_df = pd.concat(train_parts).reset_index(drop=True)\n",
    "test_df = pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "# print(f\"Training data shape: {train_df.shape}\")\n",
    "# print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Summary statistics (training data only)\n",
    "# print(\"\\nSummary statistics (training data):\")\n",
    "# print(train_df.groupby(\"Group\")[[\"GPA\", \"GMAT\"]].describe())\n",
    "\n",
    "# Scatter plot: GPA vs GMAT\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.scatterplot(\n",
    "#     data=train_df,\n",
    "#     x=\"GPA\",\n",
    "#     y=\"GMAT\",\n",
    "#     hue=\"Group\",\n",
    "#     palette=\"Set1\",\n",
    "#     s=40\n",
    "# )\n",
    "# plt.title(\"Training Data: GPA vs GMAT by Admission Group\")\n",
    "# plt.xlabel(\"Undergraduate GPA (X‚ÇÅ)\")\n",
    "# plt.ylabel(\"GMAT Score (X‚ÇÇ)\")\n",
    "# plt.grid(True)\n",
    "# # save\n",
    "# plt.savefig(\"scatter_gpa_gmat.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "\n",
    "# # Boxplots\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# sns.boxplot(data=train_df, x=\"Group\", y=\"GPA\", ax=axes[0])\n",
    "# axes[0].set_title(\"GPA by Admission Group\")\n",
    "\n",
    "# sns.boxplot(data=train_df, x=\"Group\", y=\"GMAT\", ax=axes[1])\n",
    "# axes[1].set_title(\"GMAT by Admission Group\")\n",
    "# # save\n",
    "# plt.savefig(\"boxplots_gpa_gmat.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384746-c8b7-448e-91ff-fefaf69ef694",
   "metadata": {},
   "source": [
    "### b) Perform an LDA using the training data. Superimpose the decision boundary on an appropriate display of the data. Does the decision boundary seem sensible? In addition, compute the confusion matrix and overall misclassification rate based on both training and test data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce19a70f-7641-465f-83d5-bde72ff0f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"admission.csv\")\n",
    "\n",
    "# Split into training and test data\n",
    "# last 4 observations in each group are test data\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for _, group_df in df.groupby(\"Group\"):\n",
    "    train_parts.append(group_df.iloc[:-4])\n",
    "    test_parts.append(group_df.iloc[-4:])\n",
    "\n",
    "df_train = pd.concat(train_parts).reset_index(drop=True)\n",
    "df_test = pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "X_train = df_train[[\"GPA\", \"GMAT\"]]\n",
    "y_train = df_train[\"Group\"]\n",
    "\n",
    "X_test = df_test[[\"GPA\", \"GMAT\"]]\n",
    "y_test = df_test[\"Group\"]\n",
    "\n",
    "# Perform LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[\"GPA\"].min() - 0.5, X[\"GPA\"].max() + 0.5\n",
    "    y_min, y_max = X[\"GMAT\"].min() - 50, X[\"GMAT\"].max() + 50\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h),\n",
    "        np.arange(y_min, y_max, h)\n",
    "    )\n",
    "\n",
    "    grid = pd.DataFrame(\n",
    "        np.c_[xx.ravel(), yy.ravel()],\n",
    "        columns=[\"GPA\", \"GMAT\"]\n",
    "    )\n",
    "\n",
    "    Z = model.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    # plt.scatter(\n",
    "    #     X[\"GPA\"], X[\"GMAT\"],\n",
    "    #     c=y.astype(int),\n",
    "    #     edgecolors=\"k\",\n",
    "    #     cmap=plt.cm.Paired\n",
    "    # )\n",
    "    # plt.xlabel(\"GPA\")\n",
    "    # plt.ylabel(\"GMAT\")\n",
    "    # plt.title(title)\n",
    "    # plt.savefig(\"LDA_Decision.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# Training decision boundary\n",
    "plot_decision_boundary(X_train, y_train, lda, \"LDA Decision Boundary (Training Data)\")\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "y_pred_train = lda.predict(X_train)\n",
    "y_pred_test = lda.predict(X_test)\n",
    "\n",
    "# print(\"Confusion Matrix (Test Data):\")\n",
    "# print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# print(\"\\nAccuracy (Train):\", accuracy_score(y_train, y_pred_train))\n",
    "# print(\"Accuracy (Test):\", accuracy_score(y_test, y_pred_test))\n",
    "# print(\"Misclassification Rate (Test):\", 1 - accuracy_score(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38d249-6365-4abd-800f-e21b46f8462f",
   "metadata": {},
   "source": [
    "### c) Repeat (b) using QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "521a2bc1-157a-4c17-8c1b-a278cfbecbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Perform QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Plot QDA decision boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[\"GPA\"].min() - 0.5, X[\"GPA\"].max() + 0.5\n",
    "    y_min, y_max = X[\"GMAT\"].min() - 50, X[\"GMAT\"].max() + 50\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h),\n",
    "        np.arange(y_min, y_max, h)\n",
    "    )\n",
    "\n",
    "    grid = pd.DataFrame(\n",
    "        np.c_[xx.ravel(), yy.ravel()],\n",
    "        columns=[\"GPA\", \"GMAT\"]\n",
    "    )\n",
    "\n",
    "    Z = model.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    # plt.scatter(\n",
    "    #     X[\"GPA\"], X[\"GMAT\"],\n",
    "    #     c=y.astype(int),\n",
    "    #     edgecolors=\"k\",\n",
    "    #     cmap=plt.cm.Paired\n",
    "    # )\n",
    "    # plt.xlabel(\"GPA\")\n",
    "    # plt.ylabel(\"GMAT\")\n",
    "    # plt.title(title)\n",
    "    # plt.savefig(\"QDA_Decision.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "    # plt.show()\n",
    "\n",
    "# Plot training decision boundary\n",
    "plot_decision_boundary(\n",
    "    X_train, y_train, qda,\n",
    "    \"QDA Decision Boundary (Training Data)\"\n",
    ")\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "y_pred_train = qda.predict(X_train)\n",
    "y_pred_test = qda.predict(X_test)\n",
    "\n",
    "# print(\"Confusion Matrix (Test Data):\")\n",
    "# print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# print(\"\\nAccuracy (Train):\", accuracy_score(y_train, y_pred_train))\n",
    "# print(\"Accuracy (Test):\", accuracy_score(y_test, y_pred_test))\n",
    "# print(\"Misclassification Rate (Test):\", 1 - accuracy_score(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e1bcd-bcbc-411e-bd8a-4fd67915fedd",
   "metadata": {},
   "source": [
    "### d) Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d64e0c-f561-4485-88ab-3b98426b6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Train / test split\n",
    "test_data = pd.concat([\n",
    "    df[df[\"Group\"] == 1].tail(4),\n",
    "    df[df[\"Group\"] == 2].tail(4),\n",
    "    df[df[\"Group\"] == 3].tail(4)\n",
    "])\n",
    "\n",
    "train_data = df.drop(test_data.index)\n",
    "\n",
    "X_train = train_data[[\"GPA\", \"GMAT\"]]\n",
    "y_train = train_data[\"Group\"]\n",
    "\n",
    "X_test = test_data[[\"GPA\", \"GMAT\"]]\n",
    "y_test = test_data[\"Group\"]\n",
    "\n",
    "# Standardize predictors (required for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Choose optimal K using TEST error rate\n",
    "k_range = range(1, 15)\n",
    "test_errors = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = knn.predict(X_test_scaled)\n",
    "    test_errors.append(1 - accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "optimal_k = k_range[np.argmin(test_errors)]\n",
    "# print(f\"Optimal K (chosen by test error): {optimal_k}\")\n",
    "\n",
    "# Fit optimal KNN\n",
    "knn_opt = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_opt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# TRAINING DATA performance\n",
    "y_train_pred = knn_opt.predict(X_train_scaled)\n",
    "y_train_probs = knn_opt.predict_proba(X_train_scaled)\n",
    "\n",
    "train_error = 1 - accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Sensitivity (Recall) per class\n",
    "sensitivity = np.diag(cm_train) / cm_train.sum(axis=1)\n",
    "\n",
    "# Specificity per class\n",
    "specificity = []\n",
    "for i in range(cm_train.shape[0]):\n",
    "    tn = cm_train.sum() - (cm_train[i, :].sum() + cm_train[:, i].sum() - cm_train[i, i])\n",
    "    fp = cm_train[:, i].sum() - cm_train[i, i]\n",
    "    specificity.append(tn / (tn + fp))\n",
    "\n",
    "sensitivity_macro = np.mean(sensitivity)\n",
    "specificity_macro = np.mean(specificity)\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_probs, multi_class=\"ovr\")\n",
    "\n",
    "# print(\"\\n--- TRAINING DATA PERFORMANCE ---\")\n",
    "# print(f\"Training Error Rate: {train_error:.4f}\")\n",
    "# print(f\"Sensitivity (macro-average): {sensitivity_macro:.4f}\")\n",
    "# print(f\"Specificity (macro-average): {specificity_macro:.4f}\")\n",
    "# print(f\"AUC (training): {auc_train:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. TEST DATA performance\n",
    "# --------------------------------------------------\n",
    "y_test_pred = knn_opt.predict(X_test_scaled)\n",
    "test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# print(\"\\n--- TEST DATA PERFORMANCE ---\")\n",
    "# print(f\"Estimated Test Error Rate: {test_error:.4f}\")\n",
    "\n",
    "# plot test error vs K\n",
    "# plt.plot(k_range, test_errors, marker=\"o\")\n",
    "# plt.xlabel(\"K\")\n",
    "# plt.ylabel(\"Test Error Rate\")\n",
    "# plt.title(\"KNN Test Error vs K\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfde04-bc7c-4a75-a5e5-a759d211649a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
